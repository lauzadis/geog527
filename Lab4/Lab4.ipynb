{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmtfz2PTUnex"
   },
   "source": [
    "# GEOG 527 (SP21): \n",
    "# Geospatial AI & Machine Learning\n",
    "# Lab 04: Supervised Machine Learning\n",
    "\n",
    "## (response sheet)\n",
    " \n",
    "## Due:  Sunday, 16 Apr 2021 at 11:59 PM\n",
    "## Value:  6 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfZ8svkwUsbC"
   },
   "source": [
    "# Part 1 - LULC Classification (SVM) (2 PT)\n",
    "## Follow the instructions [here ](https://github.com/giswqs/qgis-earthengine-examples) to download QGIS earth engine examples. Install QGIS Google Earth Engine plugin [how to install plugin](https://docs.qgis.org/3.4/en/docs/training_manual/qgis_plugins/fetching_plugins.html) and set up a [Google Earth Engine account](https://signup.earthengine.google.com). Note, you need to change *ee.Classifier.svm* to *ee.Classifier.libsvm* before you load the svm_classifier.py in the QGIS python console. \n",
    "\n",
    "\n",
    "*   Run the original python script (svm_classifier.py) for SVM classification and check the results in jupyter notebook. \n",
    "*   Select your own regions of interest and mannually define your own feature collection (with at least two classes) to classify the raster image. \n",
    "*   You need to briefly evaluate the model results. \n",
    "*   You also need to submit a classified raster image. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txan6hSbTV43"
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "'''\n",
    "I selected my regions of interest as two urban and two rural areas. The SVM \n",
    "Classifier was able to accurately identify the coastal areas of South America \n",
    "as more urbanized than the center of South America. This is seen in the \n",
    "screenshot linked below.\n",
    "\n",
    "In order to make the model more accurate, I would include more regions in the\n",
    "training dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/USnCTkY.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXoXrcUdgFER"
   },
   "source": [
    "# Part 2 - Remote Sensing and LULC (2 PT)\n",
    "## Based on this article - [\"*Land-Use Land-Cover Classification by Machine Learning Classifiers for Satellite Observationsâ€”A Review*\"](https://www.mdpi.com/2072-4292/12/7/1135) and briefly explain:\n",
    "\n",
    "\n",
    "1.   What are six most popular machine-learning classifiers identified by the authors?\n",
    "2.   Is there a best machine learning classifier? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwm2FkmQgM9e"
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "'''\n",
    "1. The six most popular machine-learning classifiers are random forest (RF), \n",
    "support vector machine (SVM), artificial neural network (ANN), fuzzy adaptive \n",
    "resonance theory-supervised predictive mapping (Fuzzy ARTMAP), spectral angle \n",
    "mapper (SAM), and Mahalanobis distance (MD) according to this article.\n",
    "\n",
    "2. There is no best machine learning classifier. Your use case / data source \n",
    "will determine which machine learning classifier to use, so it is dependent on \n",
    "the problem you are trying to solve. Some classifiers may be better than others \n",
    "for specific problems. The LULC article determined that each of the six models \n",
    "provided strengths and weaknesses for each LULC class. However, they determined \n",
    "that the random forest (RF) model had the best overall accuracy for highly dynamic \n",
    "areas.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udQ6KbQrmcZH"
   },
   "source": [
    "# Part 3 - Remote Sensing and Human Settlements (2 PT)\n",
    "## Based on this article - [\"*A global human settlement layer from optical HR/VHR RS data: Concept and first results*\"](https://ieeexplore.ieee.org/abstract/document/6578177) and briefly explain:\n",
    "\n",
    "\n",
    "1.   What are the datasets used in this study?\n",
    "2.   What are preprocessing methods used in image processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jK8IUw72gVa4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. The datasets used in this study include image data ranging from 0.5 to 10 \n",
    "meters, coming from satellite SPOT 2 and 5, CBERS 2B, RapidEye 2 and 4, \n",
    "WorldView 1 and 2, GeoEye 1, QuickBird 2, Ikonos 2, and airborne sensors. The \n",
    "imaging modes included panchromatic, multispectral, and pan-sharpened images.\n",
    "\n",
    "2. The positional accuracy of each scene was evaluated against a reference layer \n",
    "called TerraColor. Any images that were too skewed were discarded.\n",
    "\n",
    "Cloud coverage was evaluated using a combination of Automatical Cloud Cover \n",
    "Assessment (ACCA), and ad-hoc methods similar to it where ACCA could not be \n",
    "applied.\n",
    "\n",
    "Textural features were extracted using a gray-level co-occurrence matrix (GLCM) \n",
    "in addition to a rotation-invariant image feature called PANTEX.\n",
    "\n",
    "Morphological features were extracted using the mtDAP (Differential Attribute \n",
    "Profile) protocol which was then fed into a compression model called \n",
    "Characteristic-Saliency-Level (CSL) model.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
